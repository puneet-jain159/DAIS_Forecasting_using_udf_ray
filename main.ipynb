{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f54af6-8217-4404-afcb-abc5c9b90447",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Parallel demand forecasting at scale using Ray Tune and Pyspark UDF\n",
    "\n",
    "Batch training and tuning are common tasks in machine learning use-cases. They require training simple models, on data batches, typcially corresponding to different locations, products, etc. Batch training can take less time to process all the data at once, but only if those batches can run in parallel!\n",
    "\n",
    "This notebook showcases how to conduct batch forecasting with NeuralProphet. NeuralProphet is a popular open-source library developed by Facebook and designed for automatic forecasting of univariate time series data. \n",
    "<br></br>\n",
    "<div style=\"text-align: center; line-height: 5; padding-top: 20px;  padding-bottom: 20px;\">\n",
    "  <img src=\"https://docs.ray.io/en/master/_images/batch-training.svg\" alt='Push compute' height=\"300\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "For the data, we will use the M5 walmart dataset.This popular tabular dataset contains historical sales of products for different locations and regions in USA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea87e6ab-81a7-4e53-aa37-e9e556165fa6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ed29296-1dd4-4590-8387-254d0b9ef18d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/pkg_resources/__init__.py:122: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/databricks/python/lib/python3.9/site-packages/pkg_resources/__init__.py:122: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from datetime import timedelta, date\n",
    "import traceback\n",
    "import math\n",
    "import timeit\n",
    "import torch\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import pandas as pd\n",
    "import neuralprophet\n",
    "from neuralprophet import NeuralProphet, set_log_level\n",
    "\n",
    "\n",
    "# importing hyperopt and ray\n",
    "from hyperopt import hp\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "from ray.tune.stopper import TimeoutStopper\n",
    "from ray.tune.search.concurrency_limiter import ConcurrencyLimiter\n",
    "from ray.runtime_env import RuntimeEnv\n",
    "\n",
    "from hyperopt.pyll import scope\n",
    "from itertools import product\n",
    "from ray.util.multiprocessing import Pool\n",
    "\n",
    "import multiprocessing\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "print(num_cpus)\n",
    "\n",
    "import logging\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import Window\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "import json\n",
    "import time, os\n",
    "from ast import literal_eval\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd99876f-6bee-4edb-ab78-58063f42338d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ray Cluster Setup :\n",
    "\n",
    "There are 2 options of settig up ray cluster on databricks:\n",
    "- **Using the setup_ray_cluster API**\n",
    "  ```\n",
    "    setup_ray_cluster(\n",
    "      num_worker_nodes=MAX_NUM_WORKER_NODES,\n",
    "      num_cpus_per_node=int(num_cpu_cores_per_worker),\n",
    "      num_gpus_per_node=int(num_gpu_per_worker),\n",
    "      collect_log_to_path=RAY_LOG_DIR #ray_collected_logs\n",
    "      )\n",
    "\n",
    "    ray.init(address='auto')\n",
    "\n",
    "    num_workers = (ray.cluster_resources()[\"CPU\"]/num_cpu_cores_per_worker)\n",
    "  ```\n",
    "- **Using init scripts**\n",
    "  ```\n",
    "  #!/bin/bash\n",
    "  #RAY PORT\n",
    "  RAY_PORT=9339\n",
    "\n",
    "  # install ray\n",
    "  # Install additional ray libraries\n",
    "  /databricks/python/bin/pip install ray[tune,default]==2.5.1\n",
    "  /databricks/python/bin/pip install neuralprophet==0.6.0\n",
    "  /databricks/python/bin/pip install protobuf==3.20.0\n",
    "\n",
    "  #Create the location if not exists\n",
    "  mkdir -p /local_disk0/tmp/ray/job\n",
    "\n",
    "  # If starting on the Spark driver node, initialize the Ray head node\n",
    "  # If starting on the Spark worker node, connect to the head Ray node\n",
    "  if [ ! -z $DB_IS_DRIVER ] && [ $DB_IS_DRIVER = TRUE ] ; then\n",
    "    echo \"Starting the head node\"\n",
    "    ulimit -n 1000000\n",
    "    ray start  --head --min-worker-port=20000 --max-worker-port=25000 --temp-dir=\"/local_disk0/tmp/ray/job\"  --port=$RAY_PORT --dashboard-port=8501 --dashboard-host=\"0.0.0.0\" --include-dashboard=true --num-cpus=0 --num-gpus=0\n",
    "  else\n",
    "    sleep 40\n",
    "    ulimit -n 1000000\n",
    "    echo \"Starting the non-head node - connecting to $DB_DRIVER_IP:$RAY_PORT\"\n",
    "    ray start  --min-worker-port=20000 --max-worker-port=25000 --temp-dir=\"/local_disk0/tmp/ray/job\" --address=\"$DB_DRIVER_IP:$RAY_PORT\"  \n",
    "fi\n",
    "  ```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97425613-2df2-4e91-81aa-62df0510c847",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Visualize the ray dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee1d3167-7711-4443-802d-fc38ee172088",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./ray_dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25071795-cc69-45e3-ac02-1d725f943729",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ray.init('auto', ignore_reinit_error=True)\n",
    "ray_resources = ray.cluster_resources()\n",
    "# ray.shutdown()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bceb40f-05be-495a-899d-bcbdc9ffa390",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reading walmart data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "670ca2a4-d66c-4c22-920f-ce5df1d0d5b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf_walmart = spark.read.format('delta').load('dbfs:/walmart/data/clean_data/final_cleaned_filtered/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec7a9599-6da7-49c8-84e4-ac28aee14170",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sdf_walmart)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4adeed54-8bd3-4631-bdb1-12090dc478a6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Get The cluster information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fed952f-7d2e-4bff-bf73-9a2b60325b9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"driver_type\": \"Standard_NC8as_T4_v3\", \"worker_type\": \"Standard_NC8as_T4_v3\", \"num_workers\": 2, \"spark_ver\": \"12.2.x-gpu-ml-scala2.12\", \"cluster_memory_mb\": 172032}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "host_name = ctx.tags().get(\"browserHostName\").get()\n",
    "host_token = ctx.apiToken().get()\n",
    "cluster_id = ctx.tags().get(\"clusterId\").get()\n",
    "\n",
    "response = requests.get(\n",
    "    f'https://{host_name}/api/2.0/clusters/get?cluster_id={cluster_id}',\n",
    "    headers={'Authorization': f'Bearer {host_token}'}\n",
    "  ).json()\n",
    "cluster_info = {\n",
    "  'driver_type': response['driver_node_type_id'],\n",
    "  'worker_type': response['node_type_id'],\n",
    "  'num_workers': response['num_workers'],\n",
    "  'spark_ver': response['spark_version'],\n",
    "  'cluster_memory_mb': response['cluster_memory_mb']\n",
    "}\n",
    "cluster_info=json.dumps(cluster_info)\n",
    "print(cluster_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77bc579c-8a57-4653-91b8-ebd51c7fd9eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Create Random time-series per model for the Demo.\n",
    "make sure num_items and items_per_model are divisible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5008af4-fa90-42f4-ba94-ec873de76b1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Experiment setup - make sure num_items and items_per_model are divisible\n",
    "num_items = 200  # Max number of item time series to load, full dataset has 30490 which is overkill\n",
    "items_per_model = 100  # Number of item time series per model\n",
    "num_batches = 1  # num trials = max_concurrent_trials * num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0442ed55-de85-405e-8789-9704c4e84b4a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "window_spec = Window.orderBy('state_id', 'store_id', 'cat_id', 'dept_id', 'item_id')\n",
    "sdf_walmart_with_model_num = sdf_walmart.withColumn(\"item_num\", F.dense_rank().over(window_spec))  # A unique item number based on the window\n",
    "sdf_walmart_with_model_num = sdf_walmart_with_model_num.filter(sdf_walmart_with_model_num.item_num <= num_items)\n",
    "sdf_walmart_with_model_num = sdf_walmart_with_model_num.withColumn(\"model_num\", F.ceil(F.col(\"item_num\") / items_per_model))\n",
    "sdf_walmart_with_model_num = sdf_walmart_with_model_num.withColumn('y', F.col('sell_price')*F.col('sale_quantity'))\n",
    "sdf_walmart_with_model_num.cache()\n",
    "print(sdf_walmart.count())\n",
    "sdf_walmart_with_model_num.display()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81eaec72-129a-43f4-a22b-b34791bd0c3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Many model Forecasting with Ray Tune and Pyspark UDF\n",
    "Ray Tune is a powerful library for hyperparameter tuning, designed to simplify the development of distributed applications. It allows you to efficiently sample hyperparameters and get optimized results on your objective function. Ray Tune provides a variety of state-of-the-art hyperparameter tuning algorithms for optimizing model performance. \n",
    "\n",
    "To use Ray Tune for hyperparameter tuning, you can follow these steps:\n",
    "- Define your training function and objective function.\n",
    "- Specify the hyperparameters and their search space.\n",
    "- Define the pyspark udf function which runs ray tune for each Hierarchial model for the chosen search algorithm and scheduler.\n",
    "- Run the pyspark job and get the result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92fe1b62-29c5-4674-abe8-479f726e733a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 1 : Define the training and objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe9f497d-68e4-4dcc-a883-5f4df2d4f2f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ray_trial(config, df):\n",
    "  \"\"\"\n",
    "  Single ray trial of parameter config \n",
    "  This runs a NeuralProphet model based on the given config and then loads  \n",
    "  \"\"\"\n",
    "  print(f\"cpu_resources_per_trial : {cpu_resources_per_trial}\")\n",
    "  print(f\"num_threads : {torch.get_num_threads()}\")\n",
    "  torch.set_num_threads(int(cpu_resources_per_trial))\n",
    "  test_cutoff = df['ds'].max() - pd.Timedelta(days=7)\n",
    "  df_train = df[df['ds'] < test_cutoff]\n",
    "  df_test = df\n",
    "  trainer_config = {}\n",
    "  # Define the Model (it can be any model in our case we use NeuralProphet)\n",
    "  model = NeuralProphet(\n",
    "      accelerator='auto',\n",
    "      trainer_config=trainer_config,\n",
    "      **config\n",
    "  )\n",
    "  start = timeit.default_timer()\n",
    "  # Train model\n",
    "  progress = model.fit(\n",
    "      df=df_train,\n",
    "      checkpointing =True,\n",
    "      freq=\"D\",\n",
    "      metrics=['RMSE'],\n",
    "      progress='bar'\n",
    "    )\n",
    "  total_time = timeit.default_timer()-start\n",
    "  print(\"duration of model fit: \", total_time)\n",
    "  print(f\"df length: {df_test.shape}\")\n",
    "  # Validate the model and get the RMSE Score\n",
    "  forecast_week = model.predict(df[df['ds'] >= (df['ds'].max() - pd.Timedelta(days=360))])\n",
    "  forecast_week = forecast_week[forecast_week['ds'] >= test_cutoff]\n",
    "  forecast_week.y.fillna(0, inplace=True)\n",
    "  forecast_week.yhat1.fillna(0, inplace=True)\n",
    "  test_rmse = mean_squared_error(forecast_week.yhat1.tolist(), forecast_week.y.tolist(), squared=False)\n",
    "  d_p = progress.loc[progress['RMSE'] == progress['RMSE'].min()].to_dict(orient='records')\n",
    "  tune.report(RMSE=test_rmse,\n",
    "              Loss = d_p[0]['Loss'],\n",
    "              checkpoint = model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05d3ee0d-8432-4926-96e5-c5524cde13e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 2 : Define the search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "212f781f-a89a-4c1c-ad6b-9066b2b65dcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "space_str = \"\"\"\n",
    "{\n",
    "  \"learning_rate\": tune.uniform(0.001, 1),\n",
    "  \"n_changepoints\": 10,\n",
    "  \"n_lags\": 3, \n",
    "  'drop_missing': True,\n",
    "  'impute_rolling': 1000,\n",
    "  'newer_samples_weight': tune.uniform(1, 7),\n",
    "  'batch_size': 128,\n",
    "  \"ar_layers\": tune.choice([[64,64,64],[128,128,128],[256,256,256]]),\n",
    "  'epochs': 10\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0beb04b0-c6a6-44bb-b7dd-262d32cf8bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 3 : Define the pyspark udf function which runs ray tune for each Hierarchial model for the chosen search algorithm and scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e083fe04-9a00-4a6a-80af-512037e37f28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def udf_parallel_hpt_tune(keys, df):\n",
    "  \"\"\"\n",
    "  Single ray trial of parameter config \n",
    "  This runs a NeuralProphet model based on the given config and then loads  \n",
    "  \"\"\"\n",
    "  start = timeit.default_timer()\n",
    "  model_num = keys[0]\n",
    "  df['date_time'] = pd.to_datetime(df['date_time'], format='%Y-%m-%d')\n",
    "  df = df.sort_values(by='date_time', ascending=True)\n",
    "  df = df.rename(columns={'date_time': 'ds', 'item_num': 'ID'})\n",
    "  df = df[['ID', 'ds', 'y']]\n",
    "  space = eval(space_str)\n",
    "  ray.init(\n",
    "    address=\"auto\",\n",
    "    ignore_reinit_error=True,\n",
    "    include_dashboard=False,\n",
    "    log_to_driver=True,\n",
    "  )\n",
    "  tune_resources = {\"CPU\": cpu_resources_per_trial} if \\\n",
    "                     gpu_resources_per_trial == 0 else \\\n",
    "                      {\"CPU\": cpu_resources_per_trial,\\\n",
    "                                     \"GPU\": gpu_resources_per_trial}\n",
    "  algo = HyperOptSearch(\n",
    "    space,\n",
    "    metric='RMSE',\n",
    "    mode=\"min\",\n",
    "  )\n",
    "  scheduler = AsyncHyperBandScheduler()\n",
    "  analysis = tune.run(\n",
    "    tune.with_parameters(ray_trial, df=df),\n",
    "    search_alg=algo,\n",
    "    scheduler=scheduler,\n",
    "    metric=\"RMSE\",\n",
    "    mode=\"min\",\n",
    "    max_concurrent_trials=max_concurrent_trials,\n",
    "    num_samples=max_concurrent_trials * num_batches,\n",
    "    reuse_actors=True,\n",
    "    resources_per_trial=tune.PlacementGroupFactory(\n",
    "      [tune_resources],\n",
    "      strategy=\"PACK\"\n",
    "    ),\n",
    "  )\n",
    "  best_trial = analysis.get_best_trial(scope='last-5-avg')\n",
    "  print(f'aaa:{best_trial.last_result[\"config\"]}')\n",
    "  current_experiment=dict(mlflow.get_experiment_by_name(experiment_location))\n",
    "  experiment_id=current_experiment['experiment_id']\n",
    "  client = MlflowClient()\n",
    "  child_run = client.create_run(experiment_id = experiment_id,\n",
    "                                run_name=f\"model_{str(model_num)}\",\n",
    "                                tags={\"mlflow.parentRunId\": run_id})\n",
    "  for key ,value in best_trial.last_result['config'].items():\n",
    "    client.log_param( run_id=child_run.info.run_id,\n",
    "                      key = key,\n",
    "                      value = str(value))\n",
    "  client.log_metric( run_id=child_run.info.run_id,key = 'rmse', value = best_trial.last_result['RMSE'])\n",
    "  client.log_metric( run_id=child_run.info.run_id,key = 'Loss', value = best_trial.last_result['Loss'])\n",
    "    # with mlflow.start_run(run_name=f\"model_{str(model_num)}\" , nested=True):\n",
    "    #   mlflow.log_metric(key = 'rmse', value = best_trial.last_result['RMSE'])\n",
    "  print(f'aaa:{best_trial.last_result}')\n",
    "  best_rmse = best_trial.last_result['RMSE']\n",
    "  return pd.DataFrame([{\n",
    "    'model_num': model_num,\n",
    "    'model_HPT_time': str(timeit.default_timer()-start), \n",
    "    'num_datapoints': df['y'].count(),\n",
    "    'RMSE': best_rmse,\n",
    "    'space': space_str\n",
    "    }])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6882f6f-e237-485c-b899-2e3c0dc771e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When using pyspark udf we have to define the schema of the output generated by the udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "481865a5-fdaf-4332-93d3-9b697c26be78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "\n",
    "results_schema = StructType([\n",
    "  # StructField('state_id', StringType(), True),\n",
    "  # StructField('store_id', StringType(), True),\n",
    "  # StructField('cat_id', StringType(), True),\n",
    "  # StructField('dept_id', StringType(), True),\n",
    "  StructField('model_num', IntegerType(), True),\n",
    "  StructField('model_HPT_time', StringType(), True),\n",
    "  StructField('num_datapoints', IntegerType(), True),\n",
    "  StructField('RMSE', DoubleType(), True),\n",
    "  StructField('space', StringType(), True),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b52e639a-4bec-4218-961b-0a36b325aad6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Step 4 : Run the pyspark job and get the results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c67be35-c668-4149-ba24-8a4419e134f4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we dynamically specify the resources to be used based on the cluster choosen betweem CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "344a6323-1505-478e-8b42-c12ad92ecfd8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num models : 2\n",
      "Total concurrent Trials: 2\n",
      "gpu_resources_per_trial:1.0\n",
      "cpu_resources_per_trial:8\n",
      "***Creating DF to ray tune on 2 models with 100 item time seies per model***\n"
     ]
    }
   ],
   "source": [
    "max_concurrent_trials = 1\n",
    "num_models = sdf_walmart_with_model_num.select(F.max('model_num')).collect()[0][0]\n",
    "print(f\"num models : {num_models}\")\n",
    "total_concurrent_trials = num_models*max_concurrent_trials\n",
    "print(f\"Total concurrent Trials: {total_concurrent_trials}\")\n",
    "gpu_resources_per_trial = ray_resources['GPU']/total_concurrent_trials if 'GPU' in ray_resources.keys() else 0\n",
    "cpu_resources_per_trial = min(int(math.ceil((ray_resources['CPU']/num_models)/max_concurrent_trials)),16)\n",
    "print(f'gpu_resources_per_trial:{gpu_resources_per_trial}\\ncpu_resources_per_trial:{cpu_resources_per_trial}')\n",
    "print(f\"***Creating DF to ray tune on {num_models} models with {items_per_model} item time seies per model***\")\n",
    "# sdf_walmart_with_model_num.groupBy('state_id', 'store_id', 'cat_id', 'dept_id', 'item_id').agg(F.first('item_num').alias('item_num'), F.first('model_num').alias('model_num')).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b298f9-6df3-4229-8762-e15ce620765a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sdf_walmart_with_model_set = sdf_walmart_with_model_num.select('model_num','store_id','item_id'). \\\n",
    "                                      groupBy('model_num').agg(F.collect_set('store_id').alias('store_id'),\n",
    "                                      F.collect_set('item_id').alias('item_id'), ).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "873b51af-e2a8-4cad-be62-876097a32727",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: 2"
     ]
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.adaptive.coalescePartitions.enabled', 'false')\n",
    "spark.conf.set('spark.sql.adaptive.enabled', 'false')\n",
    "spark.conf.set('spark.databricks.optimizer.adaptive.enabled', 'false')\n",
    "spark.conf.set('spark.sql.shuffle.partitions', f'{num_models}')\n",
    "num_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89b5725a-6a7a-4816-ae83-61abb6f1135c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "experiment_location =  '/Users/puneet.jain@databricks.com/Dais_many_model_forecasting_demo'\n",
    "experiment_name = 'hpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "684b05b8-5772-4925-bdda-4f9b25ba3d26",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "get_or_create_experiment(experiment_location)\n",
    "run_id = get_new_run(experiment_location, experiment_name+\"_\"+str(date.today()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2615e09-453c-4615-bf15-71372ef87f85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>model_num</th><th>model_HPT_time</th><th>num_datapoints</th><th>RMSE</th><th>space</th></tr></thead><tbody><tr><td>2</td><td>106.74395973799994</td><td>160913</td><td>6.752816227323234</td><td>\n",
       "{\n",
       "  \"learning_rate\": tune.uniform(0.001, 1),\n",
       "  \"n_changepoints\": 10,\n",
       "  \"n_lags\": 3, \n",
       "  'drop_missing': True,\n",
       "  'impute_rolling': 300,\n",
       "  'newer_samples_weight': tune.uniform(1, 7),\n",
       "  'batch_size': 128,\n",
       "  \"ar_layers\": tune.choice([[64,64,64],[128,128,128],[256,256,256]]),\n",
       "  'epochs': 10\n",
       "}\n",
       "</td></tr><tr><td>1</td><td>110.89095562900002</td><td>168711</td><td>16.790971167565026</td><td>\n",
       "{\n",
       "  \"learning_rate\": tune.uniform(0.001, 1),\n",
       "  \"n_changepoints\": 10,\n",
       "  \"n_lags\": 3, \n",
       "  'drop_missing': True,\n",
       "  'impute_rolling': 300,\n",
       "  'newer_samples_weight': tune.uniform(1, 7),\n",
       "  'batch_size': 128,\n",
       "  \"ar_layers\": tune.choice([[64,64,64],[128,128,128],[256,256,256]]),\n",
       "  'epochs': 10\n",
       "}\n",
       "</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "106.74395973799994",
         160913,
         6.752816227323234,
         "\n{\n  \"learning_rate\": tune.uniform(0.001, 1),\n  \"n_changepoints\": 10,\n  \"n_lags\": 3, \n  'drop_missing': True,\n  'impute_rolling': 300,\n  'newer_samples_weight': tune.uniform(1, 7),\n  'batch_size': 128,\n  \"ar_layers\": tune.choice([[64,64,64],[128,128,128],[256,256,256]]),\n  'epochs': 10\n}\n"
        ],
        [
         1,
         "110.89095562900002",
         168711,
         16.790971167565026,
         "\n{\n  \"learning_rate\": tune.uniform(0.001, 1),\n  \"n_changepoints\": 10,\n  \"n_lags\": 3, \n  'drop_missing': True,\n  'impute_rolling': 300,\n  'newer_samples_weight': tune.uniform(1, 7),\n  'batch_size': 128,\n  \"ar_layers\": tune.choice([[64,64,64],[128,128,128],[256,256,256]]),\n  'epochs': 10\n}\n"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "model_num",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "model_HPT_time",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "num_datapoints",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "RMSE",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "space",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration of execution:  119.70532675099992\n"
     ]
    }
   ],
   "source": [
    "start = timeit.default_timer()\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "  results = sdf_walmart_with_model_num.repartition(num_models).\\\n",
    "            groupBy(['model_num']).applyInPandas(func=udf_parallel_hpt_tune, schema=results_schema)\n",
    "display(results.toPandas())\n",
    "total_time = timeit.default_timer()-start\n",
    "print(\"duration of execution: \", total_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3f92745-d08f-4a65-a30f-dfb5dae16b5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We see for each trial the cluster utilizes on 25% of the computation\n",
    "<br></br>\n",
    "<div style=\"text-align: center; line-height: 5; padding-top: 20px;  padding-bottom: 20px;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/puneet-jain159/Image_dump/test/ray_25_dash_utilization.png\" alt='Push compute' height=\"1000\" width=\"1600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8548c3e-32ab-41e5-8ffa-32eda725f61d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Here we see that only 25% percent of the GPU is being utlized to when running the trails"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e0c1c83-1a4a-4dfb-a9cf-44ef939c0c06",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Improving the cluster utilization\n",
    "\n",
    "Since we are consuming 25% let us increase the number of trails to 4 to utlize the GPU Cluster properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be654e2e-fd94-4577-914f-c058532a0711",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num models : 2\n",
      "Total concurrent Trials: 8\n",
      "gpu_resources_per_trial:0.25\n",
      "cpu_resources_per_trial:2\n",
      "***Creating DF to ray tune on 2 models with 100 item time seies per model***\n"
     ]
    }
   ],
   "source": [
    "max_concurrent_trials = 4\n",
    "num_models = sdf_walmart_with_model_num.select(F.max('model_num')).collect()[0][0]\n",
    "print(f\"num models : {num_models}\")\n",
    "total_concurrent_trials = num_models*max_concurrent_trials\n",
    "print(f\"Total concurrent Trials: {total_concurrent_trials}\")\n",
    "gpu_resources_per_trial = ray_resources['GPU']/total_concurrent_trials if 'GPU' in ray_resources.keys() else 0\n",
    "cpu_resources_per_trial = min(int(math.ceil((ray_resources['CPU']/num_models)/max_concurrent_trials)),16)\n",
    "print(f'gpu_resources_per_trial:{gpu_resources_per_trial}\\ncpu_resources_per_trial:{cpu_resources_per_trial}')\n",
    "print(f\"***Creating DF to ray tune on {num_models} models with {items_per_model} item time seies per model***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0fac487-41ee-40fd-a45e-2f2378273730",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>model_num</th><th>model_HPT_time</th><th>num_datapoints</th><th>RMSE</th><th>space</th></tr></thead><tbody><tr><td>2</td><td>134.20414228000004</td><td>160913</td><td>6.752816227323234</td><td>\n",
       "{\n",
       "  \"learning_rate\": tune.uniform(0.001, 1),\n",
       "  \"n_changepoints\": 10,\n",
       "  \"n_lags\": 3, \n",
       "  'drop_missing': True,\n",
       "  'impute_rolling': 300,\n",
       "  'newer_samples_weight': tune.uniform(1, 7),\n",
       "  'batch_size': 128,\n",
       "  \"ar_layers\": tune.choice([[64,64,64],[128,128,128],[256,256,256]]),\n",
       "  'epochs': 10\n",
       "}\n",
       "</td></tr><tr><td>1</td><td>139.08439712500012</td><td>168711</td><td>16.790971167565026</td><td>\n",
       "{\n",
       "  \"learning_rate\": tune.uniform(0.001, 1),\n",
       "  \"n_changepoints\": 10,\n",
       "  \"n_lags\": 3, \n",
       "  'drop_missing': True,\n",
       "  'impute_rolling': 300,\n",
       "  'newer_samples_weight': tune.uniform(1, 7),\n",
       "  'batch_size': 128,\n",
       "  \"ar_layers\": tune.choice([[64,64,64],[128,128,128],[256,256,256]]),\n",
       "  'epochs': 10\n",
       "}\n",
       "</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "134.20414228000004",
         160913,
         6.752816227323234,
         "\n{\n  \"learning_rate\": tune.uniform(0.001, 1),\n  \"n_changepoints\": 10,\n  \"n_lags\": 3, \n  'drop_missing': True,\n  'impute_rolling': 300,\n  'newer_samples_weight': tune.uniform(1, 7),\n  'batch_size': 128,\n  \"ar_layers\": tune.choice([[64,64,64],[128,128,128],[256,256,256]]),\n  'epochs': 10\n}\n"
        ],
        [
         1,
         "139.08439712500012",
         168711,
         16.790971167565026,
         "\n{\n  \"learning_rate\": tune.uniform(0.001, 1),\n  \"n_changepoints\": 10,\n  \"n_lags\": 3, \n  'drop_missing': True,\n  'impute_rolling': 300,\n  'newer_samples_weight': tune.uniform(1, 7),\n  'batch_size': 128,\n  \"ar_layers\": tune.choice([[64,64,64],[128,128,128],[256,256,256]]),\n  'epochs': 10\n}\n"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "model_num",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "model_HPT_time",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "num_datapoints",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "RMSE",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "space",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duration of execution:  147.90069438499995\n"
     ]
    }
   ],
   "source": [
    "run_id = get_new_run(experiment_location, experiment_name+\"_\"+str(date.today())+\"_max_trials_\" +str(max_concurrent_trials))\n",
    "\n",
    "start = timeit.default_timer()\n",
    "with mlflow.start_run(run_id=run_id):\n",
    "  results = sdf_walmart_with_model_num.repartition(num_models).\\\n",
    "            groupBy(['model_num']).applyInPandas(func=udf_parallel_hpt_tune, schema=results_schema)\n",
    "display(results.toPandas())\n",
    "total_time = timeit.default_timer()-start\n",
    "print(\"duration of execution: \", total_time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e192e7a-077e-4e5b-923a-8a5fcdf83f84",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We now see full utlization of the cluster\n",
    "<br></br>\n",
    "<div style=\"text-align: center; line-height: 5; padding-top: 20px;  padding-bottom: 20px;\">\n",
    "  <img src=\"https://raw.githubusercontent.com/puneet-jain159/Image_dump/test/ray_100_dash_utlization_max.png\" alt='Push compute' height=\"1000\" width=\"1600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc22591e-db76-4da1-8152-994126d86479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Demo Code",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
